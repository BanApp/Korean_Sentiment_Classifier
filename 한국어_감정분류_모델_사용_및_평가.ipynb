{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 호출 및 설정"
      ],
      "metadata": {
        "id": "kLvEazuPBEFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "id": "Sb5nh5bmB8vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMQ860bfBCYu"
      },
      "outputs": [],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch\n",
        "\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "#BERT 모델, Vocabulary 불러오기 필수\n",
        "bertmodel, vocab = get_pytorch_kobert_model()\n",
        "\n",
        "\n",
        "# KoBERT에 입력될 데이터셋 정리\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))  \n",
        "\n",
        "# 모델 정의\n",
        "class BERTClassifier(nn.Module): ## 클래스를 상속\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=6,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)\n",
        "\n",
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 20\n",
        "max_grad_norm = 1\n",
        "log_interval = 100\n",
        "learning_rate =  5e-5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#★★★현재경로가 model이 있는 폴더여야함★★★\n",
        "import os\n",
        "os.chdir('/drive/MyDrive/sentiment_calssfier_ko(epoch10,batch64)')\n",
        "\n",
        "model1 = torch.load('/drive/MyDrive/sentiment_calssfier_ko(epoch10,batch64)/6emotions_model.pt')  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
        "model1.load_state_dict(torch.load('/drive/MyDrive/sentiment_calssfier_ko(epoch10,batch64)/6emotions_model_state_dict.pt'))  # state_dict를 불러 온 후, 모델에 저장\n",
        "\n",
        "checkpoint = torch.load('/drive/MyDrive/sentiment_calssfier_ko(epoch10,batch64)/6emotions_all.tar')   # dict 불러오기\n",
        "model1.load_state_dict(checkpoint['model'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ],
      "metadata": {
        "id": "5Ijypu7rBMoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#'기쁨', '불안', '당황', '슬픔', '분노', '상처'\n",
        "\n",
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "def predict(predict_sentenc,mode):\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "\n",
        "        test_eval=[]\n",
        "        for i in out:\n",
        "            logits=i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            if np.argmax(logits) == 0:\n",
        "                test_eval.append(\"기쁨\")\n",
        "            elif np.argmax(logits) == 1:\n",
        "                test_eval.append(\"불안\")\n",
        "            elif np.argmax(logits) == 2:\n",
        "                test_eval.append(\"당황\")\n",
        "            elif np.argmax(logits) == 3:\n",
        "                test_eval.append(\"슬픔\")\n",
        "            elif np.argmax(logits) == 4:\n",
        "                test_eval.append(\"분노\")\n",
        "            elif np.argmax(logits) == 5:\n",
        "                test_eval.append(\"상처\")\n",
        "        if mode == 0:\n",
        "          ans.append(test_eval[0])\n",
        "          \n",
        "        else:\n",
        "          print(\">> 입력하신 내용에서 \" + test_eval[0] + \"이 느껴집니다.\")\n",
        "        "
      ],
      "metadata": {
        "id": "wIuoli8wBRis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 테스트 데이터 평가"
      ],
      "metadata": {
        "id": "_9Bu4EyuBTOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트셋 테스트\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "data = pd.read_csv('/content/6emotions_test.csv',encoding=\"utf-8\")\n",
        "\n",
        "ans = []\n",
        "\n",
        "for i in data['Sentences']:\n",
        "  predict(i,0)"
      ],
      "metadata": {
        "id": "QIvRubgtBVL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 입력 데이터 평가"
      ],
      "metadata": {
        "id": "BIHW_ylcBcDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end = 1\n",
        "while end == 1 :\n",
        "    sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
        "    if sentence == 0 :\n",
        "        break\n",
        "    predict(sentence,1)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "zMcuE8AzBfV9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}